{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.context import SparkContext\n",
    "from pyspark.sql import types\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf() \\\n",
    "    .setMaster('local[*]') \\\n",
    "    .setAppName('test') \\\n",
    "    .set(\"spark.jars\", \"/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
    "    .set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"/Users/amey/google/credentials/google_credentials.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"/Users/amey/google/credentials/google_credentials.json\")\n",
    "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .config(conf=sc.getConf()) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing in only necessary fields for Crashes dataset\n",
    "crashes_columns = ['CRASH_RECORD_ID','RD_NO','CRASH_DATE','POSTED_SPEED_LIMIT','WEATHER_CONDITION','LIGHTING_CONDITION','FIRST_CRASH_TYPE','STREET_NO','STREET_DIRECTION','STREET_NAME','INJURIES_TOTAL','INJURIES_FATAL']\n",
    "df_crashes_parquet = spark.read.parquet('gs://dtc_capstone_data_quiet-rigging-347402/raw/crash/*.parquet').select(crashes_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bringing in only necessary fields for Vehicles dataset\n",
    "vehicles_columns = ['CRASH_UNIT_ID','CRASH_RECORD_ID','RD_NO','CRASH_DATE','UNIT_NO','UNIT_TYPE','NUM_PASSENGERS','VEHICLE_ID','CMRC_VEH_I','MAKE','MODEL']\n",
    "df_vehicles_parquet = spark.read.parquet('gs://dtc_capstone_data_quiet-rigging-347402/raw/vehicles/*.parquet').select(vehicles_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding sequence number for each vehicle involved in a crash in Vehicles Dataset\n",
    "windowSpec  = Window.partitionBy(\"CRASH_RECORD_ID\").orderBy(\"CRASH_UNIT_ID\")\n",
    "df_vehicles_parquet= df_vehicles_parquet.withColumn(\"veh_seq_nbr\",row_number().over(windowSpec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Temp tables for Vehicles & Crashes datasets for further transformations that will be done using Spark SQL\n",
    "df_vehicles_parquet.createOrReplaceTempView('vehicles')\n",
    "df_crashes_parquet.createOrReplaceTempView('crashes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing vehicles data at crash_record_id level so it can be joined onto crashes dataset to create a final dataset to be exported back to GCP\n",
    "df_vehicle_summ = spark.sql(\"\"\"\n",
    "SELECT B.CRASH_RECORD_ID,\n",
    "    B.RD_NO,\n",
    "    B.CRASH_DATE, \n",
    "    B.Cnt_Entities_Accid,\n",
    "    B.Cnt_MotorVehicles_Accid,\n",
    "    \n",
    "    max(VEHICLE1_ID) as VEHICLE1_ID,\n",
    "    max(VEHICLE1_MAKE) as VEHICLE1_MAKE,\n",
    "    max(VEHICLE1_MODEL) as VEHICLE1_MODEL,\n",
    "    \n",
    "    max(VEHICLE2_ID) as VEHICLE2_ID,\n",
    "    max(VEHICLE2_MAKE) as VEHICLE2_MAKE,\n",
    "    max(VEHICLE2_MODEL) as VEHICLE2_MODEL,\n",
    "    \n",
    "    max(VEHICLE3_ID) as VEHICLE3_ID ,\n",
    "    max(VEHICLE3_MAKE) as VEHICLE3_MAKE,\n",
    "    max(VEHICLE3_MODEL) as VEHICLE3_MODEL ,\n",
    "    \n",
    "    max(VEHICLE4_ID) as VEHICLE4_ID ,\n",
    "    max(VEHICLE4_MAKE) as VEHICLE4_MAKE,\n",
    "    max(VEHICLE4_MODEL) as VEHICLE4_MODEL\n",
    "    FROM \n",
    "(SELECT \n",
    "    a.CRASH_RECORD_ID,\n",
    "    a.RD_NO,\n",
    "    a.CRASH_DATE, \n",
    "    \n",
    "    b1.CRASH_UNIT_ID as VEHICLE1_ID,\n",
    "    case when b1.MAKE is null then b1.UNIT_TYPE else b1.MAKE end as VEHICLE1_MAKE,\n",
    "    case when b1.MODEL is null then b1.UNIT_TYPE else b1.MODEL end as VEHICLE1_MODEL,\n",
    "    \n",
    "    b2.CRASH_UNIT_ID as VEHICLE2_ID,\n",
    "    case when b2.MAKE is null then b2.UNIT_TYPE else b2.MAKE end as VEHICLE2_MAKE,\n",
    "    case when b2.MODEL is null then b2.UNIT_TYPE else b2.MODEL end as VEHICLE2_MODEL,\n",
    "    \n",
    "    b3.CRASH_UNIT_ID as VEHICLE3_ID,\n",
    "    case when b3.MAKE is null then b3.UNIT_TYPE else b3.MAKE end as VEHICLE3_MAKE,\n",
    "    case when b3.MODEL is null then b3.UNIT_TYPE else b3.MODEL end as VEHICLE3_MODEL,\n",
    "    \n",
    "    b4.CRASH_UNIT_ID as VEHICLE4_ID,\n",
    "    case when b4.MAKE is null then b4.UNIT_TYPE else b4.MAKE end as VEHICLE4_MAKE,\n",
    "    case when b4.MODEL is null then b4.UNIT_TYPE else b4.MODEL end as VEHICLE4_MODEL,\n",
    "    \n",
    "    COUNT(a.CRASH_UNIT_ID) as Cnt_Entities_Accid,\n",
    "    COUNT(a.VEHICLE_ID) as Cnt_MotorVehicles_Accid\n",
    "    \n",
    "FROM\n",
    "    vehicles a\n",
    "    \n",
    "    left join (select CRASH_RECORD_ID, CRASH_DATE, CRASH_UNIT_ID,VEHICLE_ID,MAKE,MODEL,UNIT_TYPE\n",
    "                from vehicles where veh_seq_nbr = 1 ) b1\n",
    "    on a.CRASH_RECORD_ID = b1.CRASH_RECORD_ID\n",
    "    and a.CRASH_DATE = b1.CRASH_DATE\n",
    "    and a.CRASH_UNIT_ID = b1.CRASH_UNIT_ID\n",
    "    and a.VEHICLE_ID = b1.VEHICLE_ID\n",
    "    \n",
    "    \n",
    "    left join (select CRASH_RECORD_ID, CRASH_DATE, CRASH_UNIT_ID,VEHICLE_ID,MAKE,MODEL,UNIT_TYPE\n",
    "                from vehicles where  veh_seq_nbr = 2) b2\n",
    "    on a.CRASH_RECORD_ID = b2.CRASH_RECORD_ID\n",
    "    and a.CRASH_DATE = b2.CRASH_DATE\n",
    "    and a.CRASH_UNIT_ID = b2.CRASH_UNIT_ID\n",
    "    and a.VEHICLE_ID = b2.VEHICLE_ID\n",
    "    \n",
    "    left join (select CRASH_RECORD_ID, CRASH_DATE, CRASH_UNIT_ID,VEHICLE_ID,MAKE,MODEL,UNIT_TYPE\n",
    "                from vehicles where  veh_seq_nbr = 3) b3\n",
    "    on a.CRASH_RECORD_ID = b3.CRASH_RECORD_ID\n",
    "    and a.CRASH_DATE = b3.CRASH_DATE\n",
    "    and a.CRASH_UNIT_ID = b3.CRASH_UNIT_ID\n",
    "    and a.VEHICLE_ID = b3.VEHICLE_ID\n",
    "    \n",
    "    left join (select CRASH_RECORD_ID, CRASH_DATE, CRASH_UNIT_ID,VEHICLE_ID,MAKE,MODEL ,UNIT_TYPE\n",
    "                from vehicles where  veh_seq_nbr = 4) b4\n",
    "    on a.CRASH_RECORD_ID = b4.CRASH_RECORD_ID\n",
    "    and a.CRASH_DATE = b4.CRASH_DATE\n",
    "    and a.CRASH_UNIT_ID = b4.CRASH_UNIT_ID\n",
    "    and a.VEHICLE_ID = b4.VEHICLE_ID\n",
    "    \n",
    "group by 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15) B\n",
    "group by 1,2,3,4,5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Temp table for Vehicles Summary df created above. This temp will be used to join Vehicles Summary temp table to Crashes temp table.\n",
    "df_vehicle_summ.createOrReplaceTempView('vehicles_summ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summarizing vehicles data at crash_record_id level so it can be joined onto crashes dataset to create a final dataset to be exported back to GCP\n",
    "df_final_summ = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    A.CRASH_RECORD_ID,\n",
    "    A.RD_NO,\n",
    "    A.CRASH_DATE,\n",
    "    date_trunc('month', A.CRASH_DATE) AS CRASH_MONTH,\n",
    "    date_trunc('year', A.CRASH_DATE) AS CRASH_YEAR,\n",
    "    date_trunc('year', A.CRASH_DATE)||'_'||date_trunc('month', A.CRASH_DATE) AS YEAR_MONTH,\n",
    "    CASE WHEN EXTRACT(DOW FROM A.CRASH_DATE) = 0 THEN 'SUNDAY'\n",
    "         WHEN EXTRACT(DOW FROM A.CRASH_DATE) = 1 THEN 'MONDAY'\n",
    "         WHEN EXTRACT(DOW FROM A.CRASH_DATE) = 2 THEN 'TUESDAY'\n",
    "         WHEN EXTRACT(DOW FROM A.CRASH_DATE) = 3 THEN 'WEDNESDAY'\n",
    "         WHEN EXTRACT(DOW FROM A.CRASH_DATE) = 4 THEN 'THURSDAY'\n",
    "         WHEN EXTRACT(DOW FROM A.CRASH_DATE) = 5 THEN 'FRIDAY'\n",
    "         WHEN EXTRACT(DOW FROM A.CRASH_DATE) = 6 THEN 'SATURDAY' \n",
    "         END AS DAY_OF_WEEK,\n",
    "    EXTRACT(HOUR FROM A.CRASH_DATE) AS HOUR_OF_DAY,\n",
    "    A.POSTED_SPEED_LIMIT,\n",
    "    A.WEATHER_CONDITION,\n",
    "    A.LIGHTING_CONDITION,\n",
    "    A.FIRST_CRASH_TYPE,\n",
    "    A.STREET_NO,\n",
    "    A.STREET_DIRECTION,\n",
    "    A.STREET_NAME,\n",
    "    A.INJURIES_TOTAL,\n",
    "    A.INJURIES_FATAL,\n",
    "    B.CNT_ENTITIES_ACCID,\n",
    "    B.Cnt_MOTORVEHICLES_ACCID,\n",
    "    B.VEHICLE1_ID,\n",
    "    B.VEHICLE1_MAKE,\n",
    "    B.VEHICLE1_MODEL,\n",
    "    B.VEHICLE2_ID,\n",
    "    B.VEHICLE2_MAKE,\n",
    "    B.VEHICLE2_MODEL,\n",
    "    B.VEHICLE3_ID ,\n",
    "    B.VEHICLE3_MAKE,\n",
    "    B.VEHICLE3_MODEL ,\n",
    "    B.VEHICLE4_ID ,\n",
    "    B.VEHICLE4_MAKE,\n",
    "    B.VEHICLE4_MODEL\n",
    "\n",
    "FROM crashes A\n",
    "\n",
    "LEFT JOIN vehicles_summ B\n",
    "    ON A.CRASH_RECORD_ID = B.CRASH_RECORD_ID\n",
    "    and A.CRASH_DATE = B.CRASH_DATE\n",
    "    and A.RD_NO = B.RD_NO\n",
    "group by 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Saving parquet file to local so I can disconnect GCS connector and enable BQ connector in order upload data to BQ\n",
    "df_final_summ.write.parquet(\"/Users/Amey/Data_Engineering/dtc_capstone_proj/output/crashesdata.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-21859f82d531>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-15-21859f82d531>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    val df_final_summ = spark.read.parquet(\"/Users/Amey/Data_Engineering/dtc_capstone_proj/output/crashesdata.parquet\")\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Establishing connection with BQ\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "  .appName('1.2. BigQuery Storage & Spark SQL - Python')\\\n",
    "  .config('spark.jars', '/usr/local/Cellar/apache-spark/3.2.1/libexec/jars/spark-bigquery-with-dependencies_2.11-0.24.2.jar') \\\n",
    "  .getOrCreate()\n",
    "\n",
    "\n",
    "df_final_summ=spark.read.parquet(\"/Users/Amey/Data_Engineering/dtc_capstone_proj/output/crashesdata.parquet\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o339.save.\n: java.lang.ClassNotFoundException: \nFailed to find data source: bigquery. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:852)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 16 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a613e3d30f5e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bigquery\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"table\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"{}.{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbq_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_table\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0;34m.\u001b[0m\u001b[0moption\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"temporaryGcsBucket\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgcs_bucket\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m   \u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    736\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1322\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/apache-spark/3.2.1/libexec/python/lib/py4j-0.10.9.3-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o339.save.\n: java.lang.ClassNotFoundException: \nFailed to find data source: bigquery. Please find packages at\nhttp://spark.apache.org/third-party-projects.html\n       \n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.failedToFindDataSourceError(QueryExecutionErrors.scala:443)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:670)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSourceV2(DataSource.scala:720)\n\tat org.apache.spark.sql.DataFrameWriter.lookupV2Provider(DataFrameWriter.scala:852)\n\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:256)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:247)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.ClassNotFoundException: bigquery.DefaultSource\n\tat java.base/java.net.URLClassLoader.findClass(URLClassLoader.java:471)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:589)\n\tat java.base/java.lang.ClassLoader.loadClass(ClassLoader.java:522)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$5(DataSource.scala:656)\n\tat scala.util.Try$.apply(Try.scala:213)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$lookupDataSource$4(DataSource.scala:656)\n\tat scala.util.Failure.orElse(Try.scala:224)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.lookupDataSource(DataSource.scala:656)\n\t... 16 more\n"
     ]
    }
   ],
   "source": [
    "# Update to your GCS bucket\n",
    "gcs_bucket = 'dtc_capstone_data_quiet-rigging-347402'\n",
    "\n",
    "# Update to your BigQuery dataset name you created\n",
    "bq_dataset = 'crashes_summary'\n",
    "\n",
    "# Enter BigQuery table name you want to create or overwite. \n",
    "# If the table does not exist it will be created when you run the write function\n",
    "bq_table = 'crashes_summary_clean'\n",
    "\n",
    "df_final_summ.write \\\n",
    "  .format(\"bigquery\") \\\n",
    "  .option(\"table\",\"{}.{}\".format(bq_dataset, bq_table)) \\\n",
    "  .option(\"temporaryGcsBucket\", gcs_bucket) \\\n",
    "  .save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(CRASH_UNIT_ID=829999, CRASH_RECORD_ID='24ddf9fd8542199d832e1c223cc474e5601b356f1d77a648bb792285d3438a8d5bd177787c5cc51d347da7b1effd5920cb5ff39ce97615a184266b0a7cb615ce', RD_NO='JD124535', CRASH_DATE='01/22/2020 06:25:00 AM', UNIT_NO=1, UNIT_TYPE='DRIVER', NUM_PASSENGERS=None, VEHICLE_ID=796949, CMRC_VEH_I='', MAKE='INFINITI', MODEL='UNKNOWN', veh_seq_nbr=1)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRASH_RECORD_ID='26201e24dadcfecc3cfe69f10b7224a4df15f0baecf2e955ef8f8a161eedf196888c395ce9dd5fac43cd1d06f2111cd516400b90912744bd7bfaa0cb67ae3641', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='cb4362a84a3105a0b6b758fb5272e58c85a58f01ee2e180765100255ca8f845e36f024605e97ab9dd35c7bd6b53169f48b2702da1c62289aa5cf047a507428e0', num_vehicles=4),\n",
       " Row(CRASH_RECORD_ID='dddf911ff30ead410b91b853eeea6950051afad37014ac3ea029729b6e8af868b904db194c3fa16a1a918b882772e60166d5c26196211597b006bcc9fb7d67a4', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='c3b6d574ef99fd6240597d05f803a89dc0b202619d5b788ddf99eed81073ca2f6cada9d8359efacaac664afcd11598339ddd7b134e7cadb0c5810f826ae5f740', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='07cf72f5dc21e8251606dcebc1fd7ff511d6a1808f97f9d260203c2fe4428ec0c27e73cafb17c2addd3b35af40129524d046d71754c1ecab7318746841f56667', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='71cf510bf81b49034a82a35b8156c79afbbc2011a1233f334ac71456d57767bd424f1ae151f6121edf164223c55b8c06829d52417bd8feca0985bfcaa40de680', num_vehicles=3),\n",
       " Row(CRASH_RECORD_ID='d7c04bf62dc063491f94f13526526c4538010391187a88b4e930a3183da5176a4edaaa95456d62916a9cd5defa69336795ea1f0e07d99a74c28a3a648baa22fe', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='67af5dca6e15f9973be0ada5b3e1c8c3916b15f2426b982123f1f9d9f63fbbb30004260b0a37d0711fb182fe8db27a4de06d357671a67c076e3e86e1093d9161', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='5ed057d6ba2c2ef05d0f3323387e2c08e8fdfbd120cfcb3e36e9ddff85e14bd0e5600a55d1d5d802cea20dbd38433372c8a354095ae1b31671a27dee00cd2445', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='ff86f588ed3a0a52c5eb74e9826cada3cf91dd927eef6c763cb7916054e515337103fe843b0d41e2ae57684072a28c9edd1aa4a933aeb737f3ae5155908cd016', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='51d92121fdc7e5f7f9cf0a7588eeeef3e61ac8719100e2cad8d5e8a3edd36d4bfc580d6618c646d19d9b02056583f4e720fa13e2de03d79df06796723065ba38', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='1db886b8cc889662a10b4e549cf507d60b1fdded173d5ac11f949d01ee66b940797030770a8a793bc99018eca42d1d36223f27a119766f85222361f3c476c8db', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='97ce8f031140a9cc29dd84ca993fea5ef9dad1dad784217eddeeb992b394d543a488b9d4a62058f2cd4e6b202045cc64ff2b23cd8c6be305e7177d0cb9c5e16c', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='66ca4f07453be1dae3c1ff7a9db7edd7c15ed3cb6b5c63e3910eda33d28b9a0ec12d52bef6b9ccf65e44aedef199b651852b743acf8fd56780a471b399b69dfb', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='2e8330074204dbde1a70bb18ff1cf677075514a2a55c344573b7d0cbe377ee3537a550e59987cae1d8377bec1e4888045c21537c607926fb87332d7c3de1dd62', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='b66c9d2cd484403c1cb55e5025edc983a0468c920eae78b87e29a0e0b9338fc8a3b90dc89b81a8f361d9bc42d6558abce3d4b3c3a51b6891c465caa36e9be1d1', num_vehicles=2),\n",
       " Row(CRASH_RECORD_ID='69d5e713f8c50ec36a2db9a8bbc7e411f0262cbdae1f7cb3b8e6df660ebbdbb5c72ac7dcd371e4e681ad7c6421e8f3b5a9dcdac7e14922c9eb217d1ba397bf70', num_vehicles=3),\n",
       " Row(CRASH_RECORD_ID='921dc4b90d4fc241fc405cce7d3af309a144b5c6f8f6dc35511c2c846768322b21645f9c7102a8c92326e192c94e6d9a41d0f7d7166e769d3d4e50562060f4c3', num_vehicles=3),\n",
       " Row(CRASH_RECORD_ID='6b9a79504958399479d05c8078659474ad166e043e813fe3c834a377b99cfdb9297a7110c1ca020533c2f0e9e54d931632fd7e2a747c5afe0010c833340cf004', num_vehicles=3),\n",
       " Row(CRASH_RECORD_ID='a4a607ffdea122ea8a9406c3b6c3feb786cb9b4baf828a767561692032f64e26ad3a95db5848b4aa4d909b371bf268a8f67313516626ddcc5e4b037caffba4ef', num_vehicles=2)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRASH_UNIT_ID=940084, CRASH_RECORD_ID='cb4362a84a3105a0b6b758fb5272e58c85a58f01ee2e180765100255ca8f845e36f024605e97ab9dd35c7bd6b53169f48b2702da1c62289aa5cf047a507428e0', RD_NO='JD334931', CRASH_DATE='08/16/2020 06:30:00 PM', UNIT_NO=1, UNIT_TYPE='BICYCLE', NUM_PASSENGERS=None, VEHICLE_ID=None, CMRC_VEH_I='', MAKE='', MODEL='', veh_seq_nbr=1),\n",
       " Row(CRASH_UNIT_ID=940086, CRASH_RECORD_ID='cb4362a84a3105a0b6b758fb5272e58c85a58f01ee2e180765100255ca8f845e36f024605e97ab9dd35c7bd6b53169f48b2702da1c62289aa5cf047a507428e0', RD_NO='JD334931', CRASH_DATE='08/16/2020 06:30:00 PM', UNIT_NO=2, UNIT_TYPE='DRIVER', NUM_PASSENGERS=None, VEHICLE_ID=891177, CMRC_VEH_I='', MAKE='FORD', MODEL='MUSTANG', veh_seq_nbr=2),\n",
       " Row(CRASH_UNIT_ID=957468, CRASH_RECORD_ID='cb4362a84a3105a0b6b758fb5272e58c85a58f01ee2e180765100255ca8f845e36f024605e97ab9dd35c7bd6b53169f48b2702da1c62289aa5cf047a507428e0', RD_NO='JD334931', CRASH_DATE='08/16/2020 06:30:00 PM', UNIT_NO=2, UNIT_TYPE='BICYCLE', NUM_PASSENGERS=None, VEHICLE_ID=None, CMRC_VEH_I='', MAKE='', MODEL='', veh_seq_nbr=3),\n",
       " Row(CRASH_UNIT_ID=957469, CRASH_RECORD_ID='cb4362a84a3105a0b6b758fb5272e58c85a58f01ee2e180765100255ca8f845e36f024605e97ab9dd35c7bd6b53169f48b2702da1c62289aa5cf047a507428e0', RD_NO='JD334931', CRASH_DATE='08/16/2020 06:30:00 PM', UNIT_NO=1, UNIT_TYPE='DRIVER', NUM_PASSENGERS=None, VEHICLE_ID=907513, CMRC_VEH_I='', MAKE='FORD', MODEL='MUSTANG', veh_seq_nbr=4)]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(CRASH_RECORD_ID='cb4362a84a3105a0b6b758fb5272e58c85a58f01ee2e180765100255ca8f845e36f024605e97ab9dd35c7bd6b53169f48b2702da1c62289aa5cf047a507428e0', RD_NO='JD334931', CRASH_DATE='08/16/2020 06:30:00 PM', VEHICLE1_ID=None, VEHICLE1_MAKE=None, VEHICLE1_MODEL=None, VEHICLE2_ID=940086, VEHICLE2_MAKE='FORD', VEHICLE2_MODEL='MUSTANG', VEHICLE3_ID=None, VEHICLE3_MAKE=None, VEHICLE3_MODEL=None, VEHICLE4_ID=957469, VEHICLE4_MAKE='FORD', VEHICLE4_MODEL='MUSTANG')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
